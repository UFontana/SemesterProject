{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaiLXY2n18ko"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-MJq8oR2Eos"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP-VRfey1htD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data_path=\"/content/drive/MyDrive/semester-project-propaganda_umberto/code/data/train_df_context_emotion_prop.csv\"\n",
        "\n",
        "train_df_copy = pd.read_csv(data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lpWazXE2kI8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data_path=\"/content/drive/MyDrive/semester-project-propaganda_umberto/code/data/val_df_context_emotion_prop_fontana.csv\"\n",
        "\n",
        "val_df_copy = pd.read_csv(data_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data_path=\"/content/drive/MyDrive/semester-project-propaganda_umberto/code/data/test_df_context_emotion_prop_fontana.csv\"\n",
        "\n",
        "test_df_copy = pd.read_csv(data_path)\n"
      ],
      "metadata": {
        "id": "5o5gwFQYVQ4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data_path=\"/content/drive/MyDrive/semester-project-propaganda_umberto/code/data/train_df_context_emotion_prop_fontana.csv\"\n",
        "\n",
        "train_df_copy = pd.read_csv(data_path)\n"
      ],
      "metadata": {
        "id": "qMvG_Nm5pvXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "propaganda_techniques = ['Whataboutism',\n",
        "    'Causal_Oversimplification',\n",
        "    'Exaggeration_Minimisation',\n",
        "    'Doubt',\n",
        "    'Loaded_Language',\n",
        "    'Name_Calling_Labeling',\n",
        "    'Flag-Waving',\n",
        "    'Reductio_ad_hitlerum',\n",
        "    'Slogans',\n",
        "    'Appeal_to_fear-prejudice',\n",
        "    'Repetition',\n",
        "    'Thought-terminating_Cliches',\n",
        "    'Black-and-White_Fallacy',\n",
        "    'Bandwagon',\n",
        "    'Appeal_to_Authority',\n",
        "    'Red_Herring',\n",
        "    'Obfuscation',\n",
        "    'Straw_Men'\n",
        "]"
      ],
      "metadata": {
        "id": "5wC1ejwp0QsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3x-uXjhzFN1J"
      },
      "outputs": [],
      "source": [
        "# val_df_copy = val_df_copy.assign(label_binary=pd.Series(dtype=int))\n",
        "\n",
        "# for index, row in val_df_copy.iterrows():\n",
        "#   # print(row[\"label\"])\n",
        "#   if row[\"label\"] == \"None\":\n",
        "#     val_df_copy.at[index, \"label_binary\"] = 0\n",
        "#   else:\n",
        "#     val_df_copy.at[index, \"label_binary\"] = 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# test_df.loc[test_df['sentence_before_encoding'].apply(type) == float, 'sentence_before_encoding'] = str(np.zeros(shape=(768,)))\n",
        "# test_df.loc[test_df['sentence_after_encoding'].apply(type) == float, 'sentence_after_encoding'] = str(np.zeros(shape=(768,)))"
      ],
      "metadata": {
        "id": "Bgu4VonRjqMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(train_df_copy[\"title_similarity_scores\"][0])\n",
        "for i in train_df_copy[\"title_similarity_scores\"]:\n",
        "  # print(i)\n",
        "  # print(type(i))\n",
        "  if not isinstance(i, float):\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "KYxBTMiA2K76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4TwRUvk2X9u"
      },
      "source": [
        "## Creating datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUIdTzHm2uiF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# print(train_df_copy[\"sentence_encoding\"][0])\n",
        "def vec_to_str(str):\n",
        "  return np.fromstring(str.strip('[ ]'), sep=' ')\n",
        "\n",
        "def str_to_vec(s):\n",
        "  try:\n",
        "    array = ast.literal_eval(s)\n",
        "  except:\n",
        "    array = np.fromstring(s.strip('[ ]'), sep=' ')\n",
        "  return array\n",
        "\n",
        "\n",
        "def str_to_vec2(array_str):\n",
        "  try:\n",
        "    # Remove the brackets and split the string by commas\n",
        "    array_values = array_str.strip('[]').split(',')\n",
        "\n",
        "    # Convert the string values to float and create a NumPy array\n",
        "    array = np.array([float(value.strip()) for value in array_values])\n",
        "    return array\n",
        "  except:\n",
        "    # Split the string by spaces and convert each value to float\n",
        "    array_str = ''.join([char for char in array_str if not char.isalpha()])\n",
        "    array_values = array_str.replace(\",\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"-\", \"\").split()\n",
        "    array = np.array([float(value) for value in array_values])\n",
        "    return array\n",
        "\n",
        "\n",
        "\n",
        "train_df_copy['sentence_before_encoding'] = train_df_copy['sentence_before_encoding'].apply(lambda x: vec_to_str(x))\n",
        "train_df_copy['sentence_after_encoding'] = train_df_copy['sentence_after_encoding'].apply(lambda x: vec_to_str(x))\n",
        "train_df_copy['sentence_encoding'] = train_df_copy['sentence_encoding'].apply(lambda x: vec_to_str(x))\n",
        "train_df_copy['title_embedding'] = train_df_copy['title_embedding'].apply(lambda x: vec_to_str(x))\n",
        "train_df_copy['emotions_vector'] = train_df_copy['emotions_vector'].apply(lambda x: str_to_vec2(x))\n",
        "train_df_copy['propaganda_vector'] = train_df_copy['propaganda_vector'].apply(lambda x: str_to_vec2(x))\n",
        "train_df_copy['label_vector'] = train_df_copy['label_vector'].apply(lambda x: str_to_vec2(x))\n",
        "train_df_copy['emotion_scores_bert'] = train_df_copy['emotion_scores_bert'].apply(lambda x: str_to_vec2(x))\n",
        "\n",
        "\n",
        "\n",
        "val_df_copy['sentence_before_encoding'] = val_df_copy['sentence_before_encoding'].apply(lambda x: vec_to_str(x))\n",
        "val_df_copy['sentence_after_encoding'] = val_df_copy['sentence_after_encoding'].apply(lambda x: vec_to_str(x))\n",
        "val_df_copy['sentence_encoding'] = val_df_copy['sentence_encoding'].apply(lambda x: vec_to_str(x))\n",
        "val_df_copy['title_embedding'] = val_df_copy['title_embedding'].apply(lambda x: vec_to_str(x))\n",
        "val_df_copy['emotions_vector'] = val_df_copy['emotions_vector'].apply(lambda x: str_to_vec2(x))\n",
        "val_df_copy['propaganda_vector'] = val_df_copy['propaganda_vector'].apply(lambda x: str_to_vec2(x))\n",
        "val_df_copy['label_vector'] = val_df_copy['label_vector'].apply(lambda x: str_to_vec2(x))\n",
        "val_df_copy['emotion_scores_bert'] = val_df_copy['emotion_scores_bert'].apply(lambda x: str_to_vec2(x))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "test_df_copy['sentence_before_encoding'] = test_df_copy['sentence_before_encoding'].apply(lambda x: vec_to_str(x))\n",
        "test_df_copy['sentence_after_encoding'] = test_df_copy['sentence_after_encoding'].apply(lambda x: vec_to_str(x))\n",
        "test_df_copy['sentence_encoding'] = test_df_copy['sentence_encoding'].apply(lambda x: vec_to_str(x))\n",
        "test_df_copy['title_embedding'] = test_df_copy['title_embedding'].apply(lambda x: vec_to_str(x))\n",
        "test_df_copy['emotions_vector'] = test_df_copy['emotions_vector'].apply(lambda x: str_to_vec2(x))\n",
        "test_df_copy['propaganda_vector'] = test_df_copy['propaganda_vector'].apply(lambda x: str_to_vec2(x))\n",
        "test_df_copy['label_vector'] = test_df_copy['label_vector'].apply(lambda x: str_to_vec2(x))\n",
        "test_df_copy['emotion_scores_bert'] = test_df_copy['emotion_scores_bert'].apply(lambda x: str_to_vec2(x))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_value(df_vec, df_og, column_name):\n",
        "  for index, row in df_vec.iterrows():\n",
        "    df_vec.at[index, \"vector\"] = np.append(row[\"vector\"], df_og.loc[index, column_name])"
      ],
      "metadata": {
        "id": "Jx6UWuyo3JDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "for index, row in test_df_copy.iterrows():\n",
        "    arr = row[\"propaganda_vector\"]\n",
        "    if len(arr) != len(test_df_copy['propaganda_vector'][0]) :\n",
        "          # print(arr)\n",
        "          test_df_copy.at[index, \"propaganda_vector\"] = np.zeros(shape=(len(test_df_copy['propaganda_vector'][0]),))\n"
      ],
      "metadata": {
        "id": "y4fsZQAs7hpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOU3xuzi2qXm"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Select columns to concatenate\n",
        "# selected_columns = ['sentence_before_encoding', \"title_embedding\", \"emotions_vector\", \"propaganda_vector\"]\n",
        "selected_column_vectors = ['sentence_encoding', 'sentence_before_encoding', 'sentence_after_encoding', \"title_embedding\", \"emotions_vector\", \"emotion_scores_bert\", \"propaganda_vector\"]\n",
        "selected_column_values = [\"title_similarity_scores\", \"sentiment_scores_bert\"]\n",
        "# selected_column_values =[]\n",
        "# selected_columns = ['sentence_encoding']\n",
        "\n",
        "\n",
        "df = pd.DataFrame({'vector': []}, dtype=object)\n",
        "val_df = pd.DataFrame({'vector': []}, dtype=object)\n",
        "test_df = pd.DataFrame({'vector': []}, dtype=object)\n",
        "# df[\"vector\"] = train_df_copy[selected_columns].apply(lambda x: fun(x), axis=1)\n",
        "\n",
        "df[\"vector\"] = train_df_copy[selected_column_vectors].apply(lambda x: np.concatenate(x), axis=1)\n",
        "val_df[\"vector\"] = val_df_copy[selected_column_vectors].apply(lambda x: np.concatenate(x), axis=1)\n",
        "test_df[\"vector\"] = test_df_copy[selected_column_vectors].apply(lambda x: np.concatenate(x), axis=1)\n",
        "\n",
        "for col in selected_column_values:\n",
        "  add_value(df, train_df_copy, col)\n",
        "for col in selected_column_values:\n",
        "  add_value(val_df, val_df_copy, col)\n",
        "for col in selected_column_values:\n",
        "  add_value(test_df, test_df_copy, col)\n",
        "\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qi7uxZGPhtQ7"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "normal_len = len(val_df[\"vector\"][0])\n",
        "\n",
        "for index, row in val_df.iterrows():\n",
        "    arr = row[\"vector\"]\n",
        "    if isinstance(arr[0], str):\n",
        "          val_df.at[index, \"vector\"] = np.zeros(shape=(normal_len,))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKQdIHX1kIzR"
      },
      "outputs": [],
      "source": [
        "\n",
        "for k in df[\"vector\"]:\n",
        "  if len(k) != len(df[\"vector\"][0]):\n",
        "    print(len(k))\n",
        "\n",
        "for k in test_df[\"vector\"]:\n",
        "  if len(k) != len(test_df[\"vector\"][0]):\n",
        "    print(len(k))\n",
        "\n",
        "for index, k in val_df.iterrows():\n",
        "  if len(k[\"vector\"]) != len(val_df[\"vector\"][0]):\n",
        "    print(len(k))\n",
        "    val_df.at[index, \"vector\"] = val_df[\"vector\"][0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gmk7Qqq93DO_"
      },
      "outputs": [],
      "source": [
        "import torch.utils.data as data\n",
        "\n",
        "train_labels = torch.tensor(train_df_copy[\"label_vector\"])\n",
        "val_labels = torch.tensor(val_df_copy[\"label_vector\"])\n",
        "test_labels = torch.tensor(test_df_copy[\"label_vector\"])\n",
        "train_labels_binary = torch.tensor(train_df_copy[\"label_binary\"])\n",
        "train_labels_binary = train_labels_binary.reshape(train_labels_binary.shape[0], 1)\n",
        "val_labels_binary = torch.tensor(val_df_copy[\"label_binary\"])\n",
        "val_labels_binary = val_labels_binary.reshape(val_labels_binary.shape[0], 1)\n",
        "test_labels_binary = torch.tensor(test_df_copy[\"label_binary\"])\n",
        "test_labels_binary = test_labels_binary.reshape(test_labels_binary.shape[0], 1)\n",
        "\n",
        "\n",
        "train_data = torch.tensor(df[\"vector\"])\n",
        "val_data = torch.tensor(val_df[\"vector\"])\n",
        "test_data = torch.tensor(test_df[\"vector\"])\n",
        "\n",
        "train_dataset_multiclass = data.TensorDataset(train_data, train_labels)\n",
        "val_dataset_multiclass = data.TensorDataset(val_data, val_labels)\n",
        "test_dataset_multiclass = data.TensorDataset(test_data, test_labels)\n",
        "train_dataset_binary = data.TensorDataset(train_data, train_labels_binary)\n",
        "val_dataset_binary = data.TensorDataset(val_data, val_labels_binary)\n",
        "test_dataset_binary = data.TensorDataset(test_data, test_labels_binary)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels"
      ],
      "metadata": {
        "id": "yZzhfBbAJOhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBMR5R4jMb0g"
      },
      "outputs": [],
      "source": [
        "zeros = torch.count_nonzero(train_labels_binary == 0)\n",
        "ones = torch.count_nonzero(train_labels_binary == 1)\n",
        "positive_weight = ones / (zeros+ones)\n",
        "positive_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1vFMqpB2jHF"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "labels_list = range(1, 19)\n",
        "def get_bce_weights(train_df):\n",
        "\n",
        "    binary_labels_all_classes = []\n",
        "\n",
        "    for label in labels_list:\n",
        "        binary_labels_one_class = []\n",
        "        for row in train_df[\"label\"]:\n",
        "            labels_in_row = row.split(\",\")\n",
        "            if str(label) in labels_in_row:\n",
        "                binary_labels_one_class.append(\"1\")\n",
        "            else:\n",
        "                binary_labels_one_class.append(\"0\")\n",
        "        binary_labels_all_classes.append(binary_labels_one_class)\n",
        "        # print(f\"{current_it} / {len(train_loader)}\")\n",
        "\n",
        "    # binary_class_weights_list = torch.from_numpy(np.array(\n",
        "    #     [compute_class_weight(class_weight='balanced', classes=[\"0\", \"1\"], y=binary_labels_all_classes[i]) for i in\n",
        "    #      range(len(binary_labels_all_classes))]))\n",
        "    binary_class_weights_list = torch.from_numpy(np.array(\n",
        "        [binary_labels_all_classes[i].count(\"0\")/binary_labels_all_classes[i].count(\"1\") for i in\n",
        "         range(len(binary_labels_all_classes))]))\n",
        "    return binary_class_weights_list\n",
        "\n",
        "def get_loss_sum_weights(train_df):\n",
        "\n",
        "    y_list = []\n",
        "    for row in train_df[\"label\"]:\n",
        "        labels_arr = row.split(\",\")\n",
        "        for label in labels_arr:\n",
        "              if label != 'None':\n",
        "                y_list.append(int(label))\n",
        "\n",
        "    print(np.unique(y_list))\n",
        "    class_weights = torch.from_numpy(\n",
        "        compute_class_weight(class_weight='balanced', classes=labels_list, y=y_list)).float()\n",
        "    return class_weights\n",
        "\n",
        "def class_weight(train_df):\n",
        "    l = [0 for i in range (18)]\n",
        "    for row in train_df[\"label\"]:\n",
        "        labels_arr = row.split(\",\")\n",
        "        # print(labels_arr)\n",
        "\n",
        "        for label in labels_arr:\n",
        "              if label != 'None':\n",
        "                index = int(label)-1\n",
        "                l[index] += 1\n",
        "    s = sum(l)\n",
        "    # print(l)\n",
        "    result = [num / s for num in l]\n",
        "    return result\n",
        "\n",
        "print(class_weight(train_df_copy))\n",
        "class_weights = class_weight(train_df_copy)\n",
        "loss_sum_weights = get_loss_sum_weights(train_df_copy)\n",
        "bce_weights = get_bce_weights(train_df_copy)\n",
        "print(loss_sum_weights)\n",
        "print(bce_weights)\n",
        "criterions = [torch.nn.BCEWithLogitsLoss(pos_weight=bce_weights[i]) for i in range (len(labels_list))]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edjvux5fWsi4"
      },
      "outputs": [],
      "source": [
        "  # torch.set_grad_enabled(True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUOH60k7Jcu5"
      },
      "source": [
        "## Fully connected layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsniwN-KJwJm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "input_size = train_data.shape[1]\n",
        "\n",
        "# Step 2: Define your model for binary classification\n",
        "class BinaryModel(nn.Module):\n",
        "    def __init__(self, hidden_size=100):\n",
        "        super(BinaryModel, self).__init__()\n",
        "        self.fc = nn.Linear(input_size, hidden_size).double()\n",
        "        self.fc2 = nn.Linear(hidden_size, 1).double()\n",
        "        self.relu = nn.ReLU()\n",
        "        # self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden = self.relu(self.fc(x))\n",
        "        out = self.fc2(hidden)\n",
        "        # out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "criterion_binary = nn.BCEWithLogitsLoss(pos_weight=positive_weight)\n",
        "\n",
        "\n",
        "# Step 4: Define your model for multiclass classification\n",
        "class MulticlassModel(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(MulticlassModel, self).__init__()\n",
        "        self.fc = nn.Linear(input_size, hidden_size).double()\n",
        "        self.fc2 = nn.Linear(hidden_size, 18).double()\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden = self.relu(self.fc(x))\n",
        "        out = self.fc2(hidden)\n",
        "        # out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# Step 5: Define your loss function and optimizer for multiclass classification\n",
        "criterions = [torch.nn.BCEWithLogitsLoss(pos_weight=bce_weights[i]) for i in range (len(labels_list))]\n",
        "\n",
        "# # Step 6: Train the models\n",
        "# num_epochs = 100\n",
        "# batch_size_binary = 200\n",
        "# batch_size_multiclass = 5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--A7GNPVxHyy"
      },
      "outputs": [],
      "source": [
        "# Binary classification training loop\n",
        "\n",
        "def binary_train(learning_rate, batch_size, hidden_size):\n",
        "    binary_model = BinaryModel(hidden_size)\n",
        "    optimizer_binary = optim.Adam(binary_model.parameters(), lr=learning_rate)\n",
        "    train_data_loader_binary = data.DataLoader(train_dataset_binary, batch_size=batch_size, shuffle=True)\n",
        "    val_data_loader_binary = data.DataLoader(val_dataset_binary, batch_size=batch_size, shuffle=True)\n",
        "    best_f1 = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        binary_model.train()\n",
        "        for batch_data, batch_labels in train_data_loader_binary:\n",
        "            optimizer_binary.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = binary_model(batch_data)\n",
        "            loss = criterion_binary(outputs, batch_labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer_binary.step()\n",
        "\n",
        "        # Validation\n",
        "        binary_model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss_binary = 0.0\n",
        "            val_correct_binary = 0\n",
        "            total_samples_binary = 0\n",
        "            true_labels = []\n",
        "            predicted_labels = []\n",
        "\n",
        "            for val_data, val_labels in val_data_loader_binary:\n",
        "                m = nn.Sigmoid()\n",
        "                val_outputs = binary_model(val_data)\n",
        "                val_loss_binary += criterion_binary(val_outputs, val_labels).item()\n",
        "\n",
        "                val_outputs = m(val_outputs)\n",
        "\n",
        "                # Calculate accuracy\n",
        "                val_preds = torch.round(val_outputs)\n",
        "                val_correct_binary += torch.sum(val_preds == val_labels).item()\n",
        "                total_samples_binary += val_labels.size(0)\n",
        "                true_labels.extend(val_labels.flatten().tolist())\n",
        "                predicted_labels.extend(val_preds.flatten().tolist())\n",
        "\n",
        "            val_loss_binary /= len(val_dataset_binary)\n",
        "            val_acc_binary = val_correct_binary / total_samples_binary\n",
        "            f1 = f1_score(true_labels, predicted_labels)\n",
        "            if f1 > best_f1:\n",
        "              best_f1 = f1\n",
        "\n",
        "            # print(\"Binary Classification - Validation Loss:\", val_loss_binary)\n",
        "            # print(\"Binary Classification - Validation Accuracy:\", val_acc_binary)\n",
        "            print(f\"Binary Classification - F1: {f1}\")\n",
        "            # print(f\"Binary Classification - Precision: {precision_score(true_labels, predicted_labels)}\")\n",
        "            # print(f\"Binary Classification - Recall: {recall_score(true_labels, predicted_labels)}\")\n",
        "\n",
        "    print(f\"Best F1 score: {best_f1}\")\n",
        "    return binary_model, best_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJwBvu1h2C1C"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "# Define your hyperparameter grid\n",
        "hyperparameters = {\n",
        "    'learning_rate': [0.001, 0.005, 0.01],\n",
        "    'batch_size': [256, 512, 1024],\n",
        "    'hidden_size': [100, 200, 400]\n",
        "}\n",
        "num_epochs = 50\n",
        "\n",
        "\n",
        "# Perform grid search\n",
        "best_f1 = 0.0\n",
        "best_lr = None\n",
        "best_batch_size = None\n",
        "best_hidden_size = None\n",
        "grid_search = True\n",
        "best_model = None\n",
        "\n",
        "if grid_search:\n",
        "    grid = ParameterGrid(hyperparameters)\n",
        "    for params in grid:\n",
        "        learning_rate = params['learning_rate']\n",
        "        batch_size = params['batch_size']\n",
        "        hidden_size = params['hidden_size']\n",
        "        print(f\"Learning rate: {learning_rate}\")\n",
        "        print(f\"Batch size: {batch_size}\")\n",
        "        print(f\"Hidden size: {hidden_size}\")\n",
        "        model, f1 = binary_train(learning_rate, batch_size, hidden_size)\n",
        "        print(f\"F1 score: {f1}\")\n",
        "        if f1 > best_f1:\n",
        "          print(\"!!!!!!!!!!!!!!!!!!! BEST !!!!!!!!!!!!!!!!!!!\")\n",
        "          best_f1 = f1\n",
        "          best_lr = learning_rate\n",
        "          best_batch_size = batch_size\n",
        "          best_hidden_size = hidden_size\n",
        "          best_model = model\n",
        "\n",
        "print(f\"Best learning rate: {best_lr}\")\n",
        "print(f\"Best batch size: {best_batch_size}\")\n",
        "print(f\"Best hidden size: {best_hidden_size}\")\n",
        "print(f\"Best F1: {best_f1}\")\n",
        "\n",
        "# binary_model, _ = binary_train(0.01, 16, 200)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  #testing\n",
        "\n",
        "test_data_loader_binary = data.DataLoader(test_dataset_binary, batch_size=best_batch_size, shuffle=True)\n",
        "\n",
        "best_model.eval()\n",
        "with torch.no_grad():\n",
        "    test_loss_binary = 0.0\n",
        "    test_correct_binary = 0\n",
        "    total_samples_binary = 0\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    for test_data_, test_labels_ in test_data_loader_binary:\n",
        "        m = nn.Sigmoid()\n",
        "        test_outputs = best_model(test_data_)\n",
        "        test_loss_binary += criterion_binary(test_outputs, test_labels_).item()\n",
        "\n",
        "        test_outputs = m(test_outputs)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        test_preds = torch.round(test_outputs)\n",
        "        test_correct_binary += torch.sum(test_preds == test_labels_).item()\n",
        "        total_samples_binary += test_labels_.size(0)\n",
        "        true_labels.extend(test_labels_.flatten().tolist())\n",
        "        predicted_labels.extend(test_preds.flatten().tolist())\n",
        "\n",
        "    test_loss_binary /= len(test_dataset_binary)\n",
        "    test_acc_binary = test_correct_binary / total_samples_binary\n",
        "    f1 = f1_score(true_labels, predicted_labels)\n",
        "    if f1 > best_f1:\n",
        "      best_f1 = f1\n",
        "\n",
        "    print(f\"Binary Classification - F1: {f1}\")"
      ],
      "metadata": {
        "id": "eOy0kFhDtHg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoAC-4OUxX92"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "torch.set_grad_enabled(True)\n",
        "\n",
        "def train_multiclass(learning_rate, batch_size, hidden_size):\n",
        "    best_average_f1 = 0\n",
        "    best_weighted_f1 = 0\n",
        "    best_loaded_language_f1 = 0\n",
        "    best_binary_f1_score = 0\n",
        "    best_f1_score_arr = None\n",
        "    best_model_multiclass = None\n",
        "\n",
        "    train_data_loader_multiclass = data.DataLoader(train_dataset_multiclass, batch_size=batch_size, shuffle=True)\n",
        "    val_data_loader_multiclass = data.DataLoader(val_dataset_multiclass, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    multiclass_model = MulticlassModel(hidden_size)\n",
        "    optimizer_multiclass = optim.Adam(multiclass_model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "    # Multiclass classification training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        multiclass_model.train()\n",
        "        # binary_model.eval()\n",
        "\n",
        "        for batch_data, labels in train_data_loader_multiclass:\n",
        "            optimizer_multiclass.zero_grad()\n",
        "            # binary_outputs = binary_model(batch_data)\n",
        "\n",
        "            # index_no_propaganda = torch.round(binary_outputs)\n",
        "            # indices = torch.nonzero(index_no_propaganda.flatten() == 1).flatten()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = multiclass_model(batch_data)\n",
        "\n",
        "            # non_zero_outputs = outputs[indices, :]\n",
        "            # non_zero_labels = labels[indices, :]\n",
        "\n",
        "            loss = sum([criterions[i](outputs[:, i], labels[:, i]) for i in range(len(labels_list))])\n",
        "            # loss = sum([criterions[i](non_zero_outputs[:, i], non_zero_labels[:, i]) for i in range(len(labels_list))])\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer_multiclass.step()\n",
        "\n",
        "        # Print the loss for every epoch\n",
        "        # print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        multiclass_model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "          # Set the threshold for prediction\n",
        "          threshold = 0.5\n",
        "\n",
        "          # Initialize a list to store predicted labels and true labels\n",
        "          batch_predicted_labels = []\n",
        "          batch_true_labels = []\n",
        "          accuracy_scores = []\n",
        "          recall_scores = []\n",
        "          f1_scores = []\n",
        "          losses = []\n",
        "          binary_predicted = []\n",
        "          binary_true = []\n",
        "          all_pred_labels = []\n",
        "          all_true_labels = []\n",
        "\n",
        "          # Iterate over the test dataset\n",
        "          # corr = 0.0\n",
        "          # all = 0.0\n",
        "          for inputs, labels in val_data_loader_multiclass:  # Replace `test_dataloader` with your data loader\n",
        "              # Forward pass\n",
        "              m = nn.Sigmoid()\n",
        "\n",
        "              # binary_outputs = m(binary_model(inputs))\n",
        "              # index_propaganda = torch.round(binary_outputs)\n",
        "              # indices = torch.nonzero(index_propaganda.flatten() == 1).flatten()\n",
        "              # zero_indices = torch.nonzero(index_propaganda.flatten() == 0).flatten()\n",
        "              # print(indices)\n",
        "              # print(zero_indices)\n",
        "              outputs = multiclass_model(inputs)  # Replace `model` and `inputs` with your model and input tensors\n",
        "              # non_zero_outputs = outputs[indices, :]\n",
        "              # non_zero_labels = labels[indices, :]\n",
        "\n",
        "              # loss = sum([criterions[i](non_zero_outputs[:, i], non_zero_labels[:, i]) for i in range(len(labels_list))])\n",
        "              loss = sum([criterions[i](outputs[:, i], labels[:, i]) for i in range(len(labels_list))])\n",
        "\n",
        "              losses.append(loss)\n",
        "\n",
        "              # Apply softmax to obtain class probabilities\n",
        "              #batch*18\n",
        "              probabilities = m(outputs)\n",
        "              # probabilities[zero_indices, :] =  torch.zeros(18, dtype=torch.double)\n",
        "              # Determine predicted labels based on threshold\n",
        "              new_arr = np.copy(probabilities)\n",
        "              # Replace values based on the threshold\n",
        "              new_arr[probabilities < threshold] = 0\n",
        "              new_arr[probabilities >= threshold] = 1\n",
        "\n",
        "              # batch_true_labels.append(labels)\n",
        "              # batch_predicted_labels.append(new_arr)\n",
        "              for i, vec in enumerate(new_arr):\n",
        "                # if int(1 in new_arr[i]) == int(1 in labels[i]):\n",
        "                #   corr += 1\n",
        "                # all += 1\n",
        "                # print(int(1 in new_arr[i]) == int(1 in labels[i]))\n",
        "                binary_predicted.append(int(1 in new_arr[i]))\n",
        "                binary_true.append(int(1 in labels[i]))\n",
        "\n",
        "              # print(f\"binary acc is {corr/all}\")\n",
        "              # print(outputs.shape)\n",
        "              for i in range(labels.shape[0]):\n",
        "                  true_labels = labels[i]\n",
        "                  pred_labels = new_arr[i]\n",
        "                  # print(true_labels)\n",
        "                  # print(pred_labels)\n",
        "                  all_pred_labels.append(pred_labels.flatten().tolist())\n",
        "                  all_true_labels.append(true_labels.flatten().tolist())\n",
        "\n",
        "          # Print the metrics for each class\n",
        "          # for i in range(len(accuracy_scores)):\n",
        "          #     print(f\"Class {i+1}: Accuracy = {accuracy_scores[i]}, Recall = {recall_scores[i]}, F1 = {f1_scores[i]}\")\n",
        "          flat_all_pred_labels = list(itertools.chain.from_iterable(all_pred_labels))\n",
        "          flat_all_true_labels = list(itertools.chain.from_iterable(all_true_labels))\n",
        "          all_true_labels = np.array(all_true_labels)\n",
        "          all_pred_labels = np.array(all_pred_labels)\n",
        "          accuracy = accuracy_score(flat_all_true_labels, flat_all_pred_labels)\n",
        "          recall = recall_score(flat_all_true_labels, flat_all_pred_labels)\n",
        "          f1 = f1_score(flat_all_true_labels, flat_all_pred_labels)\n",
        "          # print(len(flat_all_true_labels))\n",
        "          # print(flat_all_pred_labels)\n",
        "          # print(flat_all_true_labels)\n",
        "          f1_scores_arr = [f1_score(all_true_labels[:, i], all_pred_labels[:, i]) for i in range (18)]\n",
        "          f1 = np.mean(f1_scores_arr)\n",
        "          # print(f\"average F1: {f1}\")\n",
        "          weighted_sum = sum(num * weight for num, weight in zip(f1_scores_arr, class_weights))\n",
        "          # print(f\"weighted F1: {weighted_sum}\" )\n",
        "          # print(f1_scores_arr)\n",
        "          f1_loaded_lang = f1_scores_arr[4]\n",
        "          # f1_score_binary = f1_score(binary_true, binary_predicted)\n",
        "          # print(f\"binary F1: {f1_score_binary}\")\n",
        "\n",
        "          if f1 > best_average_f1:\n",
        "              best_average_f1 = f1\n",
        "              best_f1_score_arr = f1_scores_arr\n",
        "              best_model_multiclass = multiclass_model\n",
        "          if weighted_sum > best_weighted_f1:\n",
        "              best_weighted_f1 = weighted_sum\n",
        "          if f1_loaded_lang > best_loaded_language_f1:\n",
        "              best_loaded_language_f1 = f1_loaded_lang\n",
        "\n",
        "    return best_model_multiclass, best_average_f1, best_weighted_f1, best_loaded_language_f1, best_f1_score_arr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhMUJbTfXIES"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "# Define your hyperparameter grid\n",
        "hyperparameters = {\n",
        "    'learning_rate': [0.0001, 0.001, 0.01],\n",
        "    'batch_size': [50, 100, 300, 600],\n",
        "    'hidden_size': [100, 200, 400]\n",
        "}\n",
        "\n",
        "num_epochs = 20\n",
        "\n",
        "# Perform grid search\n",
        "best_f1 = 0.0\n",
        "best_f1_w = 0.0\n",
        "best_f1_ll  = 0.0\n",
        "best_lr = None\n",
        "best_batch_size = None\n",
        "best_hidden_size = None\n",
        "grid_search = True\n",
        "best_f1_score_arr_ = None\n",
        "best_model_multiclass = None\n",
        "\n",
        "if grid_search:\n",
        "    grid = ParameterGrid(hyperparameters)\n",
        "    for params in grid:\n",
        "        learning_rate = params['learning_rate']\n",
        "        batch_size = params['batch_size']\n",
        "        hidden_size = params['hidden_size']\n",
        "        print(f\"Learning rate: {learning_rate}\")\n",
        "        print(f\"Batch size: {batch_size}\")\n",
        "        print(f\"Hidden size: {hidden_size}\")\n",
        "        print()\n",
        "        best_model_multilabel, f1, f1_weighted, f1_ll, best_f1_score_arr = train_multiclass(learning_rate, batch_size, hidden_size)\n",
        "        print()\n",
        "        print(f\"F1 score: {f1}\")\n",
        "        print(f\"F1 score weighted: {f1_weighted}\")\n",
        "        print(f\"F1 score loaded language: {f1_ll}\")\n",
        "\n",
        "        if f1 > best_f1:\n",
        "          print(\"!!!!!!!!!!!!!!!!!!! BEST AVERAGE !!!!!!!!!!!!!!!!!!!\")\n",
        "          best_f1 = f1\n",
        "          best_lr = learning_rate\n",
        "          best_batch_size = batch_size\n",
        "          best_hidden_size = hidden_size\n",
        "          best_f1_score_arr_ = best_f1_score_arr\n",
        "          best_model_multiclass = best_model_multilabel\n",
        "\n",
        "        if f1_weighted > best_f1_w:\n",
        "          print(\"!!!!!!!!!!!!!!!!!!! BEST WEIGHTED !!!!!!!!!!!!!!!!!!!\")\n",
        "          best_f1_w = f1_weighted\n",
        "\n",
        "        if f1_ll > best_f1_ll:\n",
        "          print(\"!!!!!!!!!!!!!!!!!!! BEST LL !!!!!!!!!!!!!!!!!!!\")\n",
        "          best_f1_ll = f1_ll\n",
        "\n",
        "\n",
        "print(f\"Best learning rate: {best_lr}\")\n",
        "print(f\"Best batch size: {best_batch_size}\")\n",
        "print(f\"Best hidden size: {best_hidden_size}\")\n",
        "print(f\"Best F1: {best_f1}\")\n",
        "print(f\"Best F1 weighted: {best_f1_w}\")\n",
        "print(f\"Best F1 loaded language: {best_f1_ll}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(propaganda_techniques)):\n",
        "  print(f\"{propaganda_techniques[i]} : {best_f1_score_arr_[i]}\")\n",
        "print(np.mean(best_f1_score_arr_))"
      ],
      "metadata": {
        "id": "d3x_76az-GB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Validation\n",
        "best_model_multiclass.eval()\n",
        "test_data_loader_multiclass = data.DataLoader(test_dataset_multiclass, batch_size=best_batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  # Set the threshold for prediction\n",
        "  threshold = 0.5\n",
        "\n",
        "  # Initialize a list to store predicted labels and true labels\n",
        "  batch_predicted_labels = []\n",
        "  batch_true_labels = []\n",
        "  accuracy_scores = []\n",
        "  recall_scores = []\n",
        "  f1_scores = []\n",
        "  losses = []\n",
        "  binary_predicted = []\n",
        "  binary_true = []\n",
        "  all_pred_labels = []\n",
        "  all_true_labels = []\n",
        "\n",
        "  # Iterate over the test dataset\n",
        "  for inputs, labels in test_data_loader_multiclass:  # Replace `test_dataloader` with your data loader\n",
        "      # Forward pass\n",
        "      m = nn.Sigmoid()\n",
        "      outputs = best_model_multiclass(inputs)  # Replace `model` and `inputs` with your model and input tensors\n",
        "\n",
        "      loss = sum([criterions[i](outputs[:, i], labels[:, i]) for i in range(len(labels_list))])\n",
        "\n",
        "      losses.append(loss)\n",
        "\n",
        "      # Apply softmax to obtain class probabilities\n",
        "      probabilities = m(outputs)\n",
        "      # Determine predicted labels based on threshold\n",
        "      new_arr = np.copy(probabilities)\n",
        "      # Replace values based on the threshold\n",
        "      new_arr[probabilities < threshold] = 0\n",
        "      new_arr[probabilities >= threshold] = 1\n",
        "\n",
        "      for i, vec in enumerate(new_arr):\n",
        "        binary_predicted.append(int(1 in new_arr[i]))\n",
        "        binary_true.append(int(1 in labels[i]))\n",
        "\n",
        "      for i in range(labels.shape[0]):\n",
        "          true_labels = labels[i]\n",
        "          pred_labels = new_arr[i]\n",
        "          all_pred_labels.append(pred_labels.flatten().tolist())\n",
        "          all_true_labels.append(true_labels.flatten().tolist())\n",
        "\n",
        "  # Print the metrics for each class\n",
        "  # for i in range(len(accuracy_scores)):\n",
        "  #     print(f\"Class {i+1}: Accuracy = {accuracy_scores[i]}, Recall = {recall_scores[i]}, F1 = {f1_scores[i]}\")\n",
        "  flat_all_pred_labels = list(itertools.chain.from_iterable(all_pred_labels))\n",
        "  flat_all_true_labels = list(itertools.chain.from_iterable(all_true_labels))\n",
        "  all_true_labels = np.array(all_true_labels)\n",
        "  all_pred_labels = np.array(all_pred_labels)\n",
        "  accuracy = accuracy_score(flat_all_true_labels, flat_all_pred_labels)\n",
        "  recall = recall_score(flat_all_true_labels, flat_all_pred_labels)\n",
        "  f1 = f1_score(flat_all_true_labels, flat_all_pred_labels)\n",
        "  f1_scores_arr = [f1_score(all_true_labels[:, i], all_pred_labels[:, i]) for i in range (18)]\n",
        "  f1 = f1_score(all_true_labels, all_pred_labels, average = \"macro\")\n",
        "  weighted_sum = sum(num * weight for num, weight in zip(f1_scores_arr, class_weights))\n",
        "  f1_loaded_lang = f1_scores_arr[4]\n",
        "\n",
        "for i in range(len(propaganda_techniques)):\n",
        "  print(f\"{propaganda_techniques[i]} : {f1_scores_arr[i]}\")\n",
        "print()\n",
        "print(f\"F1 weighted: {f1_weighted}\")\n",
        "print(f\"F1 average: {f1}\")"
      ],
      "metadata": {
        "id": "692GgKsTvI71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(len(propaganda_techniques)):\n",
        "#   print(f\"{propaganda_techniques[i]}: {best_f1_score_arr_[i]}\")"
      ],
      "metadata": {
        "id": "B0lTZQGg0XBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uHTLm61C7u8"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.data as data\n",
        "\n",
        "train_labels = torch.tensor(train_df_copy[\"label_vector\"])\n",
        "val_labels = torch.tensor(val_df_copy[\"label_vector\"])\n",
        "test_labels = torch.tensor(test_df_copy[\"label_vector\"])\n",
        "train_labels_binary = torch.tensor(train_df_copy[\"label_binary\"])\n",
        "train_labels_binary = train_labels_binary.reshape(train_labels_binary.shape[0], 1)\n",
        "val_labels_binary = torch.tensor(val_df_copy[\"label_binary\"])\n",
        "val_labels_binary = val_labels_binary.reshape(val_labels_binary.shape[0], 1)\n",
        "test_labels_binary = torch.tensor(test_df_copy[\"label_binary\"])\n",
        "test_labels_binary = test_labels_binary.reshape(test_labels_binary.shape[0], 1)\n",
        "\n",
        "\n",
        "train_data = torch.tensor(df[\"vector\"])\n",
        "val_data = torch.tensor(val_df[\"vector\"])\n",
        "test_data = torch.tensor(test_df[\"vector\"])\n",
        "\n",
        "train_dataset_multiclass = data.TensorDataset(train_data, train_labels)\n",
        "val_dataset_multiclass = data.TensorDataset(val_data, val_labels)\n",
        "test_dataset_multiclass = data.TensorDataset(test_data, test_labels)\n",
        "train_dataset_binary = data.TensorDataset(train_data, train_labels_binary)\n",
        "val_dataset_binary = data.TensorDataset(val_data, val_labels_binary)\n",
        "test_dataset_binary = data.TensorDataset(test_data, test_labels_binary)\n",
        "\n",
        "X_train = train_data\n",
        "y_train = train_labels\n",
        "X_test = test_data\n",
        "y_test = test_labels\n",
        "y_train_binary = train_labels_binary\n",
        "y_test_binary = test_labels_binary"
      ],
      "metadata": {
        "id": "7NGyEbK0-1t5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XawSWhAW2cUN"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, hamming_loss\n",
        "\n",
        "# Initialize the Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=400, random_state=42)\n",
        "rf_classifier_binary = RandomForestClassifier(n_estimators=400, random_state=42)\n",
        "\n",
        "\n",
        "# Train the Random Forest classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "rf_classifier_binary.fit(X_train, y_train_binary)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "y_pred_binary = rf_classifier_binary.predict(X_test)\n",
        "\n",
        "\n",
        "# Evaluate the performance\n",
        "# accuracy = accuracy_score(y_test, y_pred)\n",
        "# f1 = f1_score(y_test, y_pred)\n",
        "# hamming_loss_value = hamming_loss(y_test, y_pred)\n",
        "\n",
        "# print(f\"Accuracy: {accuracy}\")\n",
        "# print(f\"F1: {f1}\")\n",
        "# print(f\"Hamming Loss: {hamming_loss_value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1_weighted = f1_score(y_test, y_pred, average=\"weighted\")\n",
        "f1_average = f1_score(y_test, y_pred, average=\"macro\")\n",
        "for i in range(len(propaganda_techniques)):\n",
        "  print(f\"{propaganda_techniques[i]} : {f1_score(y_test, y_pred, average=None)[i]}\")\n",
        "print()\n",
        "print(f\"F1 weighted: {f1_weighted}\")\n",
        "print(f\"F1 average: {f1_average}\")\n",
        "print(f\"F1 binary: {f1_score(y_test_binary, y_pred_binary)}\")"
      ],
      "metadata": {
        "id": "1tGvEofQzqzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HRnpXjPC_rH"
      },
      "source": [
        "## Decision trees\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6T9D513gDC2Z"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a decision tree classifier\n",
        "clf = DecisionTreeClassifier()\n",
        "clf_binary = DecisionTreeClassifier()\n",
        "\n",
        "# Create a multi-label classifier using the decision tree as the base classifier\n",
        "# clf = MultiOutputClassifier(base_clf)\n",
        "\n",
        "# Train the multi-label classifier\n",
        "clf.fit(X_train, y_train)\n",
        "clf_binary.fit(X_train, y_train_binary)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "y_pred_binary = clf_binary.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1_weighted = f1_score(y_test, y_pred, average=\"weighted\")\n",
        "f1_average = f1_score(y_test, y_pred, average=\"macro\")\n",
        "for i in range(len(propaganda_techniques)):\n",
        "  print(f\"{propaganda_techniques[i]} : {f1_score(y_test, y_pred, average=None)[i]}\")\n",
        "print()\n",
        "print(f\"F1 weighted: {f1_weighted}\")\n",
        "print(f\"F1 average: {f1_average}\")\n",
        "print(f\"F1 binary: {f1_score(y_test_binary, y_pred_binary)}\")"
      ],
      "metadata": {
        "id": "tZ2xgunp7BJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create an SVM model with class weights\n",
        "svm = SVC(class_weight={i: bce_weights[i] for i in range(len(bce_weights))})\n",
        "svm_binary = SVC()\n",
        "\n",
        "\n",
        "# Create a multi-label classifier using one-vs-rest strategy with SVM as the base estimator\n",
        "ovr_classifier = OneVsRestClassifier(svm)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "ovr_classifier.fit(X_train, y_train)\n",
        "svm_binary.fit(X_train, y_train_binary)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = ovr_classifier.predict(X_test)\n",
        "y_pred_binary = svm_binary.predict(X_test)"
      ],
      "metadata": {
        "id": "L5kgD4QUTXxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_weighted = f1_score(y_test, y_pred, average=\"weighted\")\n",
        "f1_average = f1_score(y_test, y_pred, average=\"macro\")\n",
        "for i in range(len(propaganda_techniques)):\n",
        "  print(f\"{propaganda_techniques[i]} : {f1_score(y_test, y_pred, average=None)[i]}\")\n",
        "print()\n",
        "print(f\"F1 weighted: {f1_weighted}\")\n",
        "print(f\"F1 average: {f1_average}\")\n",
        "print(f\"F1 binary: {f1_score(y_test_binary, y_pred_binary)}\")"
      ],
      "metadata": {
        "id": "a8jnWREOnHe-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}