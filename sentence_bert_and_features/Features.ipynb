{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAhMwvAXYTO6"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install sentence_transformers\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESX6bRdrmRqZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create dataframes and add context embeddings"
      ],
      "metadata": {
        "id": "__NsCUFJtEtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path=\"/content/drive/MyDrive/semester-project-propaganda_umberto/code/data/test_df_context_emotion_prop2.csv\"\n",
        "\n",
        "test_df = pd.read_csv(data_path)"
      ],
      "metadata": {
        "id": "gkKokK5Jb7rE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3eMd7c-k00B"
      },
      "outputs": [],
      "source": [
        "PATH=\"/content/drive/MyDrive/semester-project-propaganda_umberto/code/data\"\n",
        "\n",
        "train_dir = PATH + \"/train/\"\n",
        "\n",
        "train_df = pd.DataFrame(columns=[\"id\", \"text\"])\n",
        "\n",
        "val_dir = PATH + \"/dev/\"\n",
        "val_df = pd.DataFrame(columns=[\"id\", \"text\"])\n",
        "\n",
        "test_dir = PATH + \"/test/\"\n",
        "test_df = pd.DataFrame(columns=[\"id\", \"text\"])\n",
        "\n",
        "\n",
        "# Reading of the .txt files that contains only the id and the text.\n",
        "for filename in os.listdir(train_dir):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        id = filename.split(\".\")[0][7:]\n",
        "        with open(train_dir + filename, \"r\") as f:\n",
        "            content = f.read()\n",
        "        train_df = train_df.append({\"id\": id, \"text\": content}, ignore_index=True)\n",
        "\n",
        "\n",
        "# Reading of the .txt files that contains only the id and the text.\n",
        "for filename in os.listdir(val_dir):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        id = filename.split(\".\")[0][7:]\n",
        "        with open(val_dir + filename, \"r\") as f:\n",
        "            content = f.read()\n",
        "        val_df = val_df.append({\"id\": id, \"text\": content}, ignore_index=True)\n",
        "\n",
        "# Reading of the .txt files that contains only the id and the text.\n",
        "for filename in os.listdir(test_dir):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        id = filename.split(\".\")[0][7:]\n",
        "        with open(test_dir + filename, \"r\") as f:\n",
        "            content = f.read()\n",
        "        test_df = test_df.append({\"id\": id, \"text\": content}, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTuqsDmxr4Bw"
      },
      "outputs": [],
      "source": [
        "# from transformers import BertTokenizer, BertModel\n",
        "# import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load the Sentence-BERT model\n",
        "# all-mpnet-base-v2 -> sbert\n",
        "model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "# Define a function to get the sentence encoding using Sentence-BERT\n",
        "def get_sentence_encoding(sentence):\n",
        "    # Encode the sentence\n",
        "    sentence_encoding = model.encode([sentence])\n",
        "\n",
        "    return sentence_encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rPd1rfPk107"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "def split_into_sentences2(text):\n",
        "  fixed_text = text.replace('”', '\"').replace('?\"', '\"?').replace('!\"', '\"!').replace('.\"', '\".').replace('.’\"', '’\".').replace('.’', '’.')\n",
        "  sent_text = nltk.sent_tokenize(fixed_text) # this gives us a list of sentences\n",
        "  return sent_text  \n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "train_df = pd.DataFrame(columns=['sentence_id', 'doc_id', 'start', 'end', 'text', 'label'])\n",
        "\n",
        "sent_n = 0\n",
        "for i, filename in enumerate(os.listdir(train_dir)):\n",
        "    print(f\"{i}/{len(os.listdir(train_dir))}\")\n",
        "    if filename.endswith(\".txt\"):\n",
        "        id = filename.split(\".\")[0][7:]\n",
        "        with open(train_dir + filename, \"r\") as f:\n",
        "            content = f.read()\n",
        "            sentences = split_into_sentences2(content)\n",
        "            sent_n = sent_n + len(sentences)\n",
        "            \n",
        "            output = []\n",
        "            start = 1\n",
        "            lines = content.split(\"\\n\")\n",
        "            title = lines[0]\n",
        "            # print(content)\n",
        "            # print(f\"Title: {title}\")\n",
        "            title_embedding =  get_sentence_encoding(title)\n",
        "\n",
        "            for i, sentence in enumerate(sentences):\n",
        "                sentence_before = \"\"\n",
        "                sentence_after = \"\"\n",
        "                sentence_before_encoding = None\n",
        "                sentence_after_encoding = None\n",
        "                if i > 0:\n",
        "                  sentence_before = sentences[i-1]\n",
        "                  sentence_before_encoding = get_sentence_encoding(sentence_before)\n",
        "                if i < (len(sentences)-1):\n",
        "                  sentence_after = sentences[i+1]\n",
        "                  sentence_after_encoding = get_sentence_encoding(sentence_after)\n",
        "                sentence_encoding = get_sentence_encoding(sentence)\n",
        "                # iterate over each sentence and calculate the starting and ending positions\n",
        "                start = content.find(sentence, start)\n",
        "                end = start + len(sentence) - 1\n",
        "                #df[\"id\"] = df.index + 1\n",
        "                train_df = train_df.append({'doc_id' : id, 'start' : int(start), 'end' : int(end), 'text' : sentence.strip(), 'text_before': sentence_before.strip(), 'text_after': sentence_after.strip(), 'label' : 'None', 'sentence_encoding': sentence_encoding, 'sentence_before_encoding': sentence_before_encoding, 'sentence_after_encoding': sentence_after_encoding, \"title\": title, \"title_embedding\":title_embedding}, ignore_index = True)\n",
        "                start = end + 1\n",
        "\n",
        "train_df = train_df.astype({'start':int, 'end':int})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1PTllD6A9R3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# PATH=\"/content/drive/MyDrive/semester-project-propaganda_umberto/code/data\"\n",
        "train_df.to_csv('/content/drive/MyDrive/semester-project-propaganda_umberto/code/data/train_df_context.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05mg2iah2j5x"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "def split_into_sentences2(text):\n",
        "  fixed_text = text.replace('”', '\"').replace('?\"', '\"?').replace('!\"', '\"!').replace('.\"', '\".').replace('.’\"', '’\".').replace('.’', '’.')\n",
        "  sent_text = nltk.sent_tokenize(fixed_text) # this gives us a list of sentences\n",
        "  return sent_text  \n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "val_df = pd.DataFrame(columns=['sentence_id', 'doc_id', 'start', 'end', 'text', 'label'])\n",
        "\n",
        "sent_n = 0\n",
        "for i, filename in enumerate(os.listdir(val_dir)):\n",
        "    print(f\"{i}/{len(os.listdir(val_dir))}\")\n",
        "    if filename.endswith(\".txt\"):\n",
        "        id = filename.split(\".\")[0][7:]\n",
        "        with open(val_dir + filename, \"r\") as f:\n",
        "            content = f.read()\n",
        "            sentences = split_into_sentences2(content)\n",
        "            sent_n = sent_n + len(sentences)\n",
        "            \n",
        "            output = []\n",
        "            start = 1\n",
        "            lines = content.split(\"\\n\")\n",
        "            title = lines[0]\n",
        "            # print(content)\n",
        "            # print(f\"Title: {title}\")\n",
        "            title_embedding =  get_sentence_encoding(title)\n",
        "\n",
        "            for i, sentence in enumerate(sentences):\n",
        "                sentence_before = \"\"\n",
        "                sentence_after = \"\"\n",
        "                sentence_before_encoding = None\n",
        "                sentence_after_encoding = None\n",
        "                if i > 0:\n",
        "                  sentence_before = sentences[i-1]\n",
        "                  sentence_before_encoding = get_sentence_encoding(sentence_before)\n",
        "                if i < (len(sentences)-1):\n",
        "                  sentence_after = sentences[i+1]\n",
        "                  sentence_after_encoding = get_sentence_encoding(sentence_after)\n",
        "                sentence_encoding = get_sentence_encoding(sentence)\n",
        "                # iterate over each sentence and calculate the starting and ending positions\n",
        "                start = content.find(sentence, start)\n",
        "                end = start + len(sentence) - 1\n",
        "                #df[\"id\"] = df.index + 1\n",
        "                val_df = val_df.append({'doc_id' : id, 'start' : int(start), 'end' : int(end), 'text' : sentence.strip(), 'text_before': sentence_before.strip(), 'text_after': sentence_after.strip(), 'label' : 'None', 'sentence_encoding': sentence_encoding, 'sentence_before_encoding': sentence_before_encoding, 'sentence_after_encoding': sentence_after_encoding, \"title\": title, \"title_embedding\":title_embedding}, ignore_index = True)\n",
        "                start = end + 1\n",
        "\n",
        "val_df = val_df.astype({'start':int, 'end':int})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ViQHdBX2zHK"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "def split_into_sentences2(text):\n",
        "  fixed_text = text.replace('”', '\"').replace('?\"', '\"?').replace('!\"', '\"!').replace('.\"', '\".').replace('.’\"', '’\".').replace('.’', '’.')\n",
        "  sent_text = nltk.sent_tokenize(fixed_text) # this gives us a list of sentences\n",
        "  return sent_text  \n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "test_df = pd.DataFrame(columns=['sentence_id', 'doc_id', 'start', 'end', 'text', 'label'])\n",
        "\n",
        "sent_n = 0\n",
        "for i, filename in enumerate(os.listdir(test_dir)):\n",
        "    print(f\"{i}/{len(os.listdir(test_dir))}\")\n",
        "    if filename.endswith(\".txt\"):\n",
        "        id = filename.split(\".\")[0][7:]\n",
        "        with open(test_dir + filename, \"r\") as f:\n",
        "            content = f.read()\n",
        "            sentences = split_into_sentences2(content)\n",
        "            sent_n = sent_n + len(sentences)\n",
        "            \n",
        "            output = []\n",
        "            start = 1\n",
        "            lines = content.split(\"\\n\")\n",
        "            title = lines[0]\n",
        "            # print(content)\n",
        "            # print(f\"Title: {title}\")\n",
        "            title_embedding =  get_sentence_encoding(title)\n",
        "\n",
        "            for i, sentence in enumerate(sentences):\n",
        "                sentence_before = \"\"\n",
        "                sentence_after = \"\"\n",
        "                sentence_before_encoding = None\n",
        "                sentence_after_encoding = None\n",
        "                if i > 0:\n",
        "                  sentence_before = sentences[i-1]\n",
        "                  sentence_before_encoding = get_sentence_encoding(sentence_before)\n",
        "                if i < (len(sentences)-1):\n",
        "                  sentence_after = sentences[i+1]\n",
        "                  sentence_after_encoding = get_sentence_encoding(sentence_after)\n",
        "                sentence_encoding = get_sentence_encoding(sentence)\n",
        "                # iterate over each sentence and calculate the starting and ending positions\n",
        "                start = content.find(sentence, start)\n",
        "                end = start + len(sentence) - 1\n",
        "                #df[\"id\"] = df.index + 1\n",
        "                test_df = test_df.append({'doc_id' : id, 'start' : int(start), 'end' : int(end), 'text' : sentence.strip(), 'text_before': sentence_before.strip(), 'text_after': sentence_after.strip(), 'label' : 'None', 'sentence_encoding': sentence_encoding, 'sentence_before_encoding': sentence_before_encoding, 'sentence_after_encoding': sentence_after_encoding, \"title\": title, \"title_embedding\":title_embedding}, ignore_index = True)\n",
        "                start = end + 1\n",
        "\n",
        "test_df = test_df.astype({'start':int, 'end':int})\n",
        "test_df.to_csv('/content/drive/MyDrive/semester-project-propaganda_umberto/code/data/test_df_context.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBJ5IttbldEi"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "val_df = pd.DataFrame(columns=['sentence_id', 'doc_id', 'start', 'end', 'text', 'label'])\n",
        "\n",
        "sent_n = 0\n",
        "for filename in os.listdir(val_dir):\n",
        "    \n",
        "    if filename.endswith(\".txt\"):\n",
        "        id = filename.split(\".\")[0][7:]\n",
        "        with open(val_dir + filename, \"r\") as f:\n",
        "            content = f.read()\n",
        "            sentences = split_into_sentences2(content)\n",
        "            sent_n = sent_n + len(sentences)\n",
        "            \n",
        "            output = []\n",
        "            start = 1\n",
        "\n",
        "            for sentence in sentences:\n",
        "                # iterate over each sentence and calculate the starting and ending positions\n",
        "                start = content.find(sentence, start)\n",
        "                end = start + len(sentence) - 1\n",
        "                #df[\"id\"] = df.index + 1\n",
        "                val_df = val_df.append({'doc_id' : id, 'start' : int(start), 'end' : int(end), 'text' : sentence.strip(), 'label' : 'None'}, ignore_index = True)\n",
        "                start = end + 1\n",
        "\n",
        "val_df = val_df.astype({'start':int, 'end':int})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UNJ8nO4llGj"
      },
      "outputs": [],
      "source": [
        "label_dict = {\n",
        "    'None' : 0,\n",
        "    'Whataboutism' : 1,\n",
        "    'Causal_Oversimplification' : 2,\n",
        "    'Exaggeration,Minimisation' : 3, \n",
        "    'Doubt' : 4, \n",
        "    'Loaded_Language' : 5,\n",
        "    'Name_Calling,Labeling' : 6, \n",
        "    'Flag-Waving' : 7, \n",
        "    'Reductio_ad_hitlerum' : 8,\n",
        "    'Slogans' : 9, \n",
        "    'Appeal_to_fear-prejudice' : 10, \n",
        "    'Repetition' : 11,\n",
        "    'Thought-terminating_Cliches' : 12, \n",
        "    'Black-and-White_Fallacy' : 13,\n",
        "    'Bandwagon' : 14, \n",
        "    'Appeal_to_Authority' : 15, \n",
        "    'Red_Herring' : 16,\n",
        "    'Obfuscation,Intentional_Vagueness,Confusion' : 17, \n",
        "    'Straw_Men' : 18\n",
        "}\n",
        "\n",
        "id_dict = {\n",
        "    0:'None',\n",
        "    1:'Whataboutism',\n",
        "    2:'Causal_Oversimplification',\n",
        "    3:'Exaggeration,Minimisation', \n",
        "    4:'Doubt', \n",
        "    5:'Loaded_Language',\n",
        "    6:'Name_Calling,Labeling', \n",
        "    7:'Flag-Waving', \n",
        "    8:'Reductio_ad_hitlerum',\n",
        "    9:'Slogans', \n",
        "    10:'Appeal_to_fear-prejudice', \n",
        "    11:'Repetition',\n",
        "    12:'Thought-terminating_Cliches', \n",
        "    13:'Black-and-White_Fallacy',\n",
        "    14:'Bandwagon', \n",
        "    15:'Appeal_to_Authority', \n",
        "    16:'Red_Herring',\n",
        "    17:'Obfuscation,Intentional_Vagueness,Confusion', \n",
        "    18:'Straw_Men'\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "def label_name_to_id(text):\n",
        "  labels = \"\"\n",
        "  first = True\n",
        "  for key in label_dict:\n",
        "    if key in text:\n",
        "      if not first:\n",
        "        labels += \",\"\n",
        "        print(key)\n",
        "      labels += str(label_dict[key])\n",
        "      first = False\n",
        "  return labels\n",
        "\n",
        "print(label_name_to_id(\"None\"))\n",
        "\n",
        "\n",
        "def id_to_label(text):\n",
        "  labels = \"\"\n",
        "  first = True\n",
        "  arr = text.split(\",\")\n",
        "  for key in arr:\n",
        "    if not first:\n",
        "      labels += \",\"\n",
        "      # print(key)\n",
        "    labels += id_dict[int(key)]\n",
        "    first = False\n",
        "  return labels\n",
        "\n",
        "id_to_label(\"1,2,3\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVWPSOxAlzd8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "train_df_copy = train_df.copy()\n",
        "val_df_copy = val_df.copy()\n",
        "\n",
        "\n",
        "technique_sizes = [[] for i in range (len(id_dict))]\n",
        "\n",
        "\n",
        "for filename in os.listdir(train_dir):\n",
        "    if filename.endswith(\".tsv\"):\n",
        "        tmp_df = pd.read_csv(train_dir + filename, sep='\\t', names = ['id', 'type', 'start', 'end'])\n",
        "        id = filename.split(\".\")[0][7:]\n",
        "        \n",
        "        #tmp_df = tmp_df.astype({'start' : int, 'end' : int})\n",
        "\n",
        "        for _, row in tmp_df.iterrows():\n",
        "          tech_id = label_dict[row[\"type\"]]\n",
        "          technique_sizes[tech_id].append(int(row['end']) - int(row['start']))\n",
        "          v = train_df_copy.loc[(train_df_copy['doc_id'] == id) & \n",
        "                           ((train_df_copy['start'] <= row['start']) & #p starts after beginning of the sentence and before the ending\n",
        "                           (train_df_copy['end'] >= row['start']) |\n",
        "                           (train_df_copy['start'] <= row['end']) & #p starts after beginning of the sentence and before the ending\n",
        "                           (train_df_copy['end'] >= row['end']))] \n",
        "\n",
        "          \n",
        "\n",
        "          if len(train_df_copy.loc[v.index, 'label'].values) == 0:\n",
        "            continue\n",
        "          current_label = train_df_copy.loc[v.index, 'label'].values[0]\n",
        "          if current_label == \"None\":\n",
        "            train_df_copy.loc[v.index, 'label'] = str(label_dict[row[\"type\"]])\n",
        "          else:\n",
        "            train_df_copy.loc[v.index, 'label'] = str(current_label) + \",\" + str(label_dict[row[\"type\"]])\n",
        "        \n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "technique_sizes_val = [[] for i in range (len(id_dict))]\n",
        "\n",
        "\n",
        "for filename in os.listdir(val_dir):\n",
        "    if filename.endswith(\".tsv\"):\n",
        "        tmp_df = pd.read_csv(val_dir + filename, sep='\\t', names = ['id', 'type', 'start', 'end'])\n",
        "        id = filename.split(\".\")[0][7:]\n",
        "        \n",
        "        #tmp_df = tmp_df.astype({'start' : int, 'end' : int})\n",
        "\n",
        "        for _, row in tmp_df.iterrows():\n",
        "          tech_id = label_dict[row[\"type\"]]\n",
        "          technique_sizes_val[tech_id].append(int(row['end']) - int(row['start']))\n",
        "          v = val_df_copy.loc[(val_df_copy['doc_id'] == id) & \n",
        "                           ((val_df_copy['start'] <= row['start']) & #p starts after beginning of the sentence and before the ending\n",
        "                           (val_df_copy['end'] >= row['start']) |\n",
        "                           (val_df_copy['start'] <= row['end']) & #p starts after beginning of the sentence and before the ending\n",
        "                           (val_df_copy['end'] >= row['end']))] \n",
        "\n",
        "          \n",
        "\n",
        "          if len(val_df_copy.loc[v.index, 'label'].values) == 0:\n",
        "            continue\n",
        "          current_label = val_df_copy.loc[v.index, 'label'].values[0]\n",
        "          if current_label == \"None\":\n",
        "            val_df_copy.loc[v.index, 'label'] = str(label_dict[row[\"type\"]])\n",
        "          else:\n",
        "            val_df_copy.loc[v.index, 'label'] = str(current_label) + \",\" + str(label_dict[row[\"type\"]])\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "technique_sizes_test = [[] for i in range (len(id_dict))]\n",
        "\n",
        "for filename in os.listdir(test_dir):\n",
        "    if filename.endswith(\".tsv\"):\n",
        "        tmp_df = pd.read_csv(test_dir + filename, sep='\\t', names = ['id', 'type', 'start', 'end'])\n",
        "        id = filename.split(\".\")[0][7:]\n",
        "        \n",
        "        #tmp_df = tmp_df.astype({'start' : int, 'end' : int})\n",
        "\n",
        "        for _, row in tmp_df.iterrows():\n",
        "          tech_id = label_dict[row[\"type\"]]\n",
        "          technique_sizes_test[tech_id].append(int(row['end']) - int(row['start']))\n",
        "          v = test_df.loc[(test_df['doc_id'] == id) & \n",
        "                           ((test_df['start'] <= row['start']) & #p starts after beginning of the sentence and before the ending\n",
        "                           (test_df['end'] >= row['start']) |\n",
        "                           (test_df['start'] <= row['end']) & #p starts after beginning of the sentence and before the ending\n",
        "                           (test_df['end'] >= row['end']))] \n",
        "\n",
        "          \n",
        "\n",
        "          if len(test_df.loc[v.index, 'label'].values) == 0:\n",
        "            continue\n",
        "          current_label = test_df.loc[v.index, 'label'].values[0]\n",
        "          if current_label == \"None\":\n",
        "            test_df.loc[v.index, 'label'] = str(label_dict[row[\"type\"]])\n",
        "          else:\n",
        "            test_df.loc[v.index, 'label'] = str(current_label) + \",\" + str(label_dict[row[\"type\"]])\n",
        "   "
      ],
      "metadata": {
        "id": "02nChvnITjlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add emotion features"
      ],
      "metadata": {
        "id": "wkVoB7iEtawC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4mVI5ahkoDK"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install openai\n",
        "!pip install sentence_transformers\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfdXIHBl28gY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data_path=\"/content/drive/MyDrive/semester-project-propaganda_umberto/code/data/train_df_context.csv\"\n",
        "\n",
        "train_df_copy = pd.read_csv(data_path)\n",
        "\n",
        "train_df_copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Df9P5vjT0lWM"
      },
      "outputs": [],
      "source": [
        "emotions = [\n",
        "    \"Fear\",\n",
        "    \"Anger\",\n",
        "    \"Anxiety\",\n",
        "    \"Suspicion\",\n",
        "    \"Hatred\",\n",
        "    \"Paranoia\",\n",
        "    \"Resentment\",\n",
        "    \"Hostility\",\n",
        "    \"Disgust\",\n",
        "    \"Contempt\",\n",
        "    \"Bitterness\",\n",
        "    \"Cynicism\",\n",
        "    \"Apathy\",\n",
        "    \"Despair\",\n",
        "    \"Pessimism\",\n",
        "    \"Resignation\",\n",
        "    \"Sorrow\",\n",
        "    \"Regret\",\n",
        "    \"Guilt\",\n",
        "    \"Shame\",\n",
        "    \"Embarrassment\",\n",
        "    \"Humiliation\",\n",
        "    \"Insecurity\",\n",
        "    \"Vulnerability\",\n",
        "    \"Intimidation\",\n",
        "    \"Worry\",\n",
        "    \"Panic\",\n",
        "    \"Confusion\",\n",
        "    \"Distress\",\n",
        "    \"Amusement\",\n",
        "    \"Joy\",\n",
        "    \"Delight\",\n",
        "    \"Excitement\",\n",
        "    \"Satisfaction\",\n",
        "    \"Pride\",\n",
        "    \"Compassion\",\n",
        "    \"Empathy\",\n",
        "    \"Trust\",\n",
        "    \"Love\",\n",
        "    \"Affection\",\n",
        "    \"Admiration\",\n",
        "    \"Respect\",\n",
        "    \"Hope\",\n",
        "    \"Optimism\",\n",
        "    \"Enthusiasm\",\n",
        "    \"Inspiration\",\n",
        "    \"Motivation\",\n",
        "    \"Fascination\",\n",
        "    \"Surprise\",\n",
        "    \"Excitement\",\n",
        "    \"Regard\",\n",
        "    \"Belongingness\",\n",
        "    \"Patriotism\",\n",
        "    \"Pride\",\n",
        "    \"Unity\",\n",
        "    \"Authority\",\n",
        "    \"Prejudice\",\n",
        "    \"Doubt\",\n",
        "    \"Nationalism\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIyKu4BbYRw9"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "# Set up your OpenAI API credentials\n",
        "openai.api_key = 'api'\n",
        "\n",
        "def get_vector(tokens, probs):\n",
        "  # arr = np.zeros(shape=(len(emotions),))\n",
        "  arr = [0 for i in range(len(emotions))]\n",
        "  for i, token in enumerate(tokens):\n",
        "    token = token.strip()\n",
        "    prob = probs[i].strip()\n",
        "    try:\n",
        "      ind = emotions.index(token)\n",
        "      arr[ind] = prob\n",
        "    except Exception as e:\n",
        "    # Handling the exception\n",
        "      print(\"An exception occurred:\", e)\n",
        "  return arr\n",
        "\n",
        "\n",
        "# Define a function to perform multi-class classification\n",
        "def detect_emotions(text, emotions):\n",
        "    # Prompt for the classification task\n",
        "    # prompt = \"This is an emotion detection model specifically trained to analyze text for propaganda-related content. It is designed for multi-class text classification. Please provide some text for emotion detection:\\n\\nText: \" + text + \"\\n\\nOptions: \" + ', '.join(emotions) + \"\\n\\nClasses:\"\n",
        "\n",
        "        # Define the system message\n",
        "    system_msg = 'You are a helpful assistant who performs emotion detection specifically trained to analyze text for propaganda-related content. You are designed for multi-class text classification. For each given sentence return detected emotions from the possible options and also return the probability of each emotion in form of a python array. '\n",
        "\n",
        "    # Define the user message\n",
        "    user_msg = \"Detect emotions in next sentence: \\\"In the course of their conversations, Patel discussed his desire to see a holy war between Muslims and non-Muslims.\\\"\"+ \"\\nOptions: \" + ', '.join(emotions)\n",
        "\n",
        "    assistant_msg = \"Anger, Hostility, Paranoia, Worry\\nProbabilities: [0.54, 0.34, 0.11, 0.05]\"\n",
        "\n",
        "    try:\n",
        "    # Create a dataset using GPT\n",
        "      response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n",
        "                                              messages=[\n",
        "          {\"role\": \"system\", \"content\": system_msg},\n",
        "          {\"role\": \"user\", \"content\": user_msg},\n",
        "          {\"role\": \"assistant\", \"content\": assistant_msg},\n",
        "          {\"role\": \"user\", \"content\": \"Detect emotions in the next sentence: \\n\\\"\"+text+\"\\\"\\nOptions: \"+ ', '.join(emotions)}\n",
        "      ])\n",
        "\n",
        "      # Extract the generated classifications from the response\n",
        "      # token_logprobs = response.choices[0].logprobs.token_logprobs\n",
        "      tokens = response.choices[0].message.content.split(\"\\n\")[0].split(\",\")\n",
        "      probs = response.choices[0].message.content.split(\"Probabilities:\")[1].replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")\n",
        "      vec = get_vector(tokens, probs)\n",
        "      return vec\n",
        "    except Exception as e:\n",
        "    # Handling the exception\n",
        "      print(\"An exception occurred:\", e)\n",
        "      # print(response)\n",
        "# Example usage\n",
        "text_to_classify = \"To no one’s surprise, Trump’s legal tormentor, Stormy Daniels attorney Michael Avenatti, whom Tucker Carlson has dubbed \\“creepy porn lawyer,\\\" is representing Swetnick.\"\n",
        "\n",
        "# class_options = ['Option 1', 'Option 2', 'Option 3']  # List of possible classes\n",
        "\n",
        "\n",
        "# Perform multi-class classification\n",
        "classified_labels = detect_emotions(text_to_classify, emotions)\n",
        "print(\"Classified labels:\", classified_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOcnxA2FTHDB"
      },
      "outputs": [],
      "source": [
        "test_df = test_df.assign(emotions_vector=pd.Series(dtype=object))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sfYa-H9tiJG"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "for index, row in test_df.iterrows():\n",
        "    print(f\"{index}/{test_df.shape[0]}\")\n",
        "    if isinstance(row[\"emotions_vector\"], float) and math.isnan(row[\"emotions_vector\"]) or row[\"emotions_vector\"]==\"nan\":\n",
        "        try:\n",
        "          if index % 1000 == 0:\n",
        "            test_df.to_csv('/content/drive/MyDrive/semester-project-propaganda_umberto/code/data/test_df_ontext_emotion.csv', index=False)\n",
        "\n",
        "          value = detect_emotions(row[\"text\"], emotions)\n",
        "          print(f\"Vector is: {value}\")\n",
        "          test_df.at[index, 'emotions_vector'] =  value\n",
        "        except Exception as e:\n",
        "          # Handling the exception\n",
        "          print(\"An exception occurred:\", e)\n",
        "\n",
        "test_df.to_csv('/content/drive/MyDrive/semester-project-propaganda_umberto/code/data/test_df_ontext_emotion.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcpWD8Jfpi0J"
      },
      "outputs": [],
      "source": [
        "train_df_context = train_df.to_csv('/content/drive/MyDrive/semester-project-propaganda_umberto/code/data/train_df_context.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVQySEMFvQBp"
      },
      "outputs": [],
      "source": [
        "#add emotions\n",
        "\n",
        "train_df_copy = train_df.copy()\n",
        "\n",
        "for index, row in train_df.iterrows():\n",
        "    train_df_copy.at[index, 'emotions'] = new_column_value\n",
        "    train_df_copy.at[index, 'emotions_encoding'] = new_column_value"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add propaganda techniques"
      ],
      "metadata": {
        "id": "Kezqt00Atnhf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GlhpIACKtmIS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnnOx0_vpdWJ"
      },
      "outputs": [],
      "source": [
        "propaganda_techniques = ['Whataboutism',\n",
        "    'Causal_Oversimplification',\n",
        "    'Exaggeration_Minimisation', \n",
        "    'Doubt', \n",
        "    'Loaded_Language',\n",
        "    'Name_Calling_Labeling', \n",
        "    'Flag-Waving', \n",
        "    'Reductio_ad_hitlerum',\n",
        "    'Slogans', \n",
        "    'Appeal_to_fear-prejudice', \n",
        "    'Repetition',\n",
        "    'Thought-terminating_Cliches', \n",
        "    'Black-and-White_Fallacy',\n",
        "    'Bandwagon', \n",
        "    'Appeal_to_Authority', \n",
        "    'Red_Herring',\n",
        "    'Obfuscation', \n",
        "    'Straw_Men'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GwuybZYD9lN"
      },
      "outputs": [],
      "source": [
        "instructions = ['1. Whataboutism:  Discredit an opponent’s position by charging them with hypocrisy without directly disproving their argument (Richter, 2017). For example, mentioning an event that discredits the opponent: “What about . . . ?” (Richter, 2017). \\n examples: \\\"President Trump —who himself avoided national military service in the 1960’s— keeps beating the war drums over North Korea.\\\" ; \"Russia Today had a proclivity for whataboutism in its coverage of the 2015 Baltimore and Ferguson protests in the US, which revealed a consistent refrain: “the oppression of blacks in the US has become so unbearable that the eruption of violence was inevitable”, and that the US therefore lacks “the moral high ground to discuss human rights issues in countries like Russia and China.”\"',\n",
        "    '2. Causal_Oversimplification: Assuming a single cause or reason when there are multiple causes behind an issue. We include in the definition also scapegoating, i.e. the transfer of the blame to one person or group of people without investigating the complexities of an issue.\\n example: \\\"If France had not have declared war on Germany then World War II would have never happened.\\\"',\n",
        "    '3. Exaggeration_Minimisation: Either representing something in an excessive manner: making things larger, better, worse or making something seem less important or smaller than it actually is, e.g., saying that an insult was just a joke (Jowett and O’Donnell, 2012b, pag. 303).\\n examples: \\\"Coronavirus ‘risk to the American people remains very low’, Trump said.\\\", “Democrats bolted as soon as Trump’s speech ended in an apparent effort to signal they can’t even stomach being in the same room as the president”; “I was not fighting with her; we were just playing.”', \n",
        "    '4. Doubt:  Questioning the credibility of someone or something.\\n examples: \\\"Can the same be said for the Obama Administration?\\\"; A candidate says about his opponent: “Is he ready to be the Mayor?”', \n",
        "    '5. Loaded_Language: Using specific words and phrases with strong emotional implications (either positive or negative) to influence an audience (Weston, 2000, p. 6).\\n Examples: \\\"Outrage as Donald Trump suggests injecting disinfectant to kill virus.\\\"; “[. . . ] a lone lawmaker’s childish shouting.”',\n",
        "    '6. Name_Calling_Labeling: Labeling the object of the propaganda campaign as either something the target audience fears, hates, finds undesirable or loves, praises (Miller, 1939).\\n Examples:  “Republican congressweasels”; “Bush the Leser.”;  \\\"WHO: Coronavirus emergency is ’Public Enemy Number 1’\\\"', \n",
        "    '7. Flag-Waving:  Playing on strong national feeling (or with respect to any group, e.g., race, gender, political preference) to justify or promote an action or idea (Hobbs and Mcgee, 2008).\\n examples: \\\"Mueller attempts to stop the will of We the People!!! It’s time to jail Mueller.\\\"; “entering this war will make us have a better future in our country.”', \n",
        "    '8. Reductio_ad_hitlerum:  Persuading an audience to disapprove an action or idea by suggesting that it is popular with groups hated in contempt by the target audience. It can refer to any person or concept with a negative connotation (Teninbaum, 2009).\\n examples: “Only one kind of person can think this way: a communist.”; \\\"“Vichy journalism,” a term which now fits so much of the mainstream media. It collaborates in the same way that the Vichy government in France collaborated with the Nazis.\\\"',\n",
        "    '9. Slogans:  A brief and striking phrase that may include labeling and stereotyping. Slogans tend to act as emotional appeals (Dan, 2015).\\n examples: \\\"“BUILD THE WALL!” Trump tweeted.\\\"; “Make America great again!”', \n",
        "    '10. Appeal_to_fear-prejudice: Seeking to build support for an idea by instilling anxiety and/or panic in the population towards an alternative, possibly based on preconceived judgments.\\n examples: \\\"A dark, impenetrable and “irreversible” winter of persecution of the faithful by their own shepherds will fall.\\\"; “stop those refugees; they are terrorists.”', \n",
        "    '11. Repetition: Repeating the same message over and over again, so that the audience will eventually accept it (Torok, 2015; Miller, 1939). \\n example: \\\"I still have a dream. It is a dream deeply rooted in the American dream. I have a dream that one day . . .\\\"',\n",
        "    '12. Thought-terminating_Cliches:  Words or phrases that discourage critical thought and meaningful discussion on a topic. They are typically short, generic sentences that offer seemingly simple answers to complex questions or that distract attention away from other lines of thought (Hunter, 2015, p. 78).\\n example: \\\"I do not really see any problems there. Marx is the President.\\\"; “it is what it is”; “you cannot judge it without experiencing it”; “it’s common sense”, “nothing is permanent except change”, “better late than never”; “mind your own business”; “nobody’s perfect”; “it doesn’t matter”; “you can’t change human nature.”', \n",
        "    '13. Black-and-White_Fallacy:  Presenting two alternative options as the only possibilities, when in fact more possibilities exist (Torok, 2015). Dictatorship is an extreme case: telling the audience exactly what actions to take, eliminating any other possible choice. \\n examples: \\\" Francis said these words: “Everyone is guilty for the good he could have done and did not do . . . If we do not oppose evil, we tacitly feed it.”\\\"; “You must be a Republican or Democrat; you are not a Democrat. Therefore, you must be a Republican”; “There is no alternative to war.”',\n",
        "    '14. Bandwagon:  Attempting to persuade the target audience to join in and take the course of action because “everyone else is taking the same action” (Hobbs and Mcgee, 2008).\\n example: \\\"He tweeted, “EU no longer considers #Hamas a terrorist group. Time for US to do same.”\\\"; “Would you vote for Clinton as president? 57% say yes.”', \n",
        "    '15. Appeal_to_Authority:  Stating that a claim is true simply because a valid authority or expert on the issue supports it, without any other supporting evidence (Goodwin, 2011). We include in this technique the special case in which the reference is not an authority or an expert, although it is referred to as testimonial in the literature (Jowett and O’Donnell, 2012b, pag. 237).\\n example: \\\"Monsignor Jean-Franois Lantheaume, who served as first Counsellor of the Nunciature in Washington, confirmed that “Vigan said the truth. That’s all.”\\\"', \n",
        "    '16. Red_Herring:Introducing irrelevant material to the issue being discussed, so that everyone’s attention is diverted away from the points made (Weston, 2018, p. 78). Those subjected to a red herring argument are led away from the issue that had been the focus of the discussion and urged to follow an observation or claim that may be associated with the original claim, but is not highly relevant to the issue in dispute (Teninbaum, 2009).\\n examples: “You may claim that the death penalty is an ineffective deterrent against crime – but what about the victims of crime? How do you think surviving family members feel when they see the man who murdered their son kept in prison at their expense? Is it right that they should pay for their son’s murderer to be fed and housed?”; \\\"You may claim that the death penalty is an ineffective deterrent against crime – but what about the victims of crime? How do you think surviving family members feel when they see the man who murdered their son kept in prison at their expense? Is it right that they should pay for their son’s murderer to be fed and housed?”\\\"',\n",
        "    '17. Obfuscation: Using deliberately unclear words, so that the audience may have its own interpretation (Suprabandari, 2007; Weston, 2018, p. 8). For instance, when an unclear phrase with multiple possible meanings is used within the argument, and, therefore, it does not really support the conclusion.\\n examples: “It is a good idea to listen to victims of theft. Therefore, if the victims say to have the thief shot, then you should do it.”; \\\"The cardinal’s office maintains that rather than saying “yes,” there is a possibility of liturgical “blessing” of gay unions, he answered the question in a more subtle way without giving an explicit “yes.”\\\"', \n",
        "    '18. Straw_Men:  When an opponent’s proposition is substituted with a similar one which is then refuted in place of the original (Walton, 2013). Weston (2000, p. 78) specifies the characteristics of the substituted proposition: “caricaturing an opposing view so that it is easy to refute”.\\n example: \\\"“Take it seriously, but with a large grain of salt.” Which is just Allen’s more nuanced way of saying: “Don’t believe it.”\\\"'\n",
        "]\n",
        "\n",
        "for i in instructions:\n",
        "  print(i)\n",
        "  print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAUmRhMaDckG"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import time\n",
        "\n",
        "# Set up your OpenAI API credentials\n",
        "openai.api_key = 'api'\n",
        "\n",
        "def get_vector(tokens, probs):\n",
        "  # arr = np.zeros(shape=(len(emotions),))\n",
        "  arr = [0 for i in range(len(propaganda_techniques))]\n",
        "  if len(probs)==1:\n",
        "    print(\"just one\")\n",
        "    ind = propaganda_techniques.index(tokens[0])\n",
        "    arr[ind] = probs[0]\n",
        "    return arr\n",
        "\n",
        "  for i, token in enumerate(tokens):\n",
        "    token = token.strip()\n",
        "    prob = probs[i].strip()\n",
        "    try:\n",
        "      ind = propaganda_techniques.index(token)\n",
        "      arr[ind] = prob\n",
        "    except Exception as e:\n",
        "    # Handling the exception\n",
        "      print(\"An exception occurred:\", e)\n",
        "  return arr\n",
        "\n",
        "\n",
        "# Define a function to perform multi-class classification\n",
        "def detect_techniques(text, propaganda_techniques):\n",
        "    # Prompt for the classification task\n",
        "    # prompt = \"This is an emotion detection model specifically trained to analyze text for propaganda-related content. It is designed for multi-class text classification. Please provide some text for emotion detection:\\n\\nText: \" + text + \"\\n\\nOptions: \" + ', '.join(emotions) + \"\\n\\nClasses:\"\n",
        "    time.sleep(0.7)\n",
        "        # Define the system message\n",
        "    system_msg = 'You are a helpful assistant who performs multi label classification for propaganda techniques. For each given sentence return detected propaganda techniques from the possible options and also return the probability of each technique in a form of a python array. Possible propaganda techniques with explanations and examples: \\n' + '\\n '.join(instructions) + \"\\nPlease make sure you write the name of the classes exactly as in the list I provided.\"\n",
        "\n",
        "    # Define the user message\n",
        "    user_msg = \"Detect propaganda techniques in following sentence: \\\" Instead, the tough-talking politician said they were regarded as “Muslim invaders\\\".\"+ \"\\nOptions: \" + ', '.join(propaganda_techniques) + \" \\nPlease make sure you write the name of the classes exactly as in the list I provided. If you don't detect any propaganda technique, please write exactly: \\\"No detected propaganda in this sentence.\\\"\"\n",
        "\n",
        "    # assistant_msg = \"Anger, Hostility, Paranoia, Worry\\nProbabilities: [0.54, 0.34, 0.11, 0.05]\"\n",
        "    assistant_msg = \"Name_Calling_Labeling, Appeal_to_Authority, Appeal_to_fear-prejudice, Loaded_Language\\nProbabilities: [0.84, 0.09, 0.67, 0.87]\"\n",
        "    try:\n",
        "    # Create a dataset using GPT\n",
        "      response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n",
        "                                              messages=[\n",
        "          {\"role\": \"system\", \"content\": system_msg},\n",
        "          {\"role\": \"user\", \"content\": user_msg},\n",
        "          {\"role\": \"assistant\", \"content\": assistant_msg},\n",
        "          {\"role\": \"user\", \"content\": \"Detect propaganda techniques in the next sentence: \\n\\\"\"+text+\"\\\"\\nOptions: \"+ ', '.join(propaganda_techniques)}\n",
        "      ])\n",
        "\n",
        "      print(response.choices[0].message.content)\n",
        "      # print(type(response.choices[0].message.content))\n",
        "      mess = response.choices[0].message.content\n",
        "      if \"No detected propaganda\" in mess:\n",
        "        return [0 for k in range(18)]\n",
        "\n",
        "      # Extract the generated classifications from the response\n",
        "      # token_logprobs = response.choices[0].logprobs.token_logprobs\n",
        "      tokens = response.choices[0].message.content.split(\"\\n\")[0].split(\",\")\n",
        "      try:        \n",
        "        probs = response.choices[0].message.content.split(\"Probabilities:\")[1].replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")\n",
        "      except:\n",
        "        probs = response.choices[0].message.content.split(\"Probability:\")[1].replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")\n",
        "      print(f\"tokens: {tokens}\")\n",
        "      print(type(tokens))\n",
        "      print(probs)\n",
        "      vec = get_vector(tokens, probs)\n",
        "      return vec\n",
        "    except Exception as e:\n",
        "    # Handling the exception\n",
        "      print(\"An exception occurred:\", e)\n",
        "      # print(response)\n",
        "# Example usage\n",
        "text_to_classify = \"To no one’s surprise, Trump’s legal tormentor, Stormy Daniels attorney Michael Avenatti, whom Tucker Carlson has dubbed \\“creepy porn lawyer,\\\" is representing Swetnick.\"\n",
        "# text_to_classify = \"Hi, im Jana\"\n",
        "# class_options = ['Option 1', 'Option 2', 'Option 3']  # List of possible classes\n",
        "\n",
        "\n",
        "# Perform multi-class classification\n",
        "classified_labels = detect_techniques(text_to_classify, propaganda_techniques)\n",
        "print(\"Classified labels:\", classified_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyF5zP0SJgNq"
      },
      "outputs": [],
      "source": [
        "test_df = test_df.assign(propaganda_vector=pd.Series(dtype=object))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsLXhD5RHz5y"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "for index, row in test_df.iterrows():\n",
        "    print(f\"{index}/{test_df.shape[0]}\")\n",
        "    if(isinstance(row[\"propaganda_vector\"], float) and math.isnan(row[\"propaganda_vector\"]) or row[\"propaganda_vector\"]==\"None\" or row[\"propaganda_vector\"] is None):\n",
        "      try:\n",
        "        if index % 1000 == 0 and index != 0:\n",
        "          test_df.to_csv('/content/drive/MyDrive/semester-project-propaganda_umberto/code/data/test_df_context_emotion_prop.csv', index=False)\n",
        "\n",
        "        value = detect_techniques(row[\"text\"], propaganda_techniques)\n",
        "        print(f\"Vector is: {value}\")\n",
        "        test_df.at[index, 'propaganda_vector'] =  value\n",
        "      except Exception as e:\n",
        "        # Handling the exception\n",
        "        print(\"An exception occurred:\", e)\n",
        "\n",
        "test_df.to_csv('/content/drive/MyDrive/semester-project-propaganda_umberto/code/data/test_df_context_emotion_prop.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eIwx4OEHnAs"
      },
      "outputs": [],
      "source": [
        "test_df.to_csv('/content/drive/MyDrive/semester-project-propaganda_umberto/code/data/test_df_context_emotion_prop.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-cmxfaw4uZ1"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "# Set up your OpenAI API credentials\n",
        "openai.api_key = 'api'\n",
        "\n",
        "# Define a function to perform multi-class classification\n",
        "def detect_techniques(text, techniques):\n",
        "    # Prompt for the classification task\n",
        "    prompt = \"This is model for recognizing propaganda techniques in a sentence. It is designed for multi-class text classification. Please provide some text for propaganda technique detection:\\n\\nText: \" + text + \"\\n\\nOptions: \" + ', '.join(techniques) + \"\\n\\nClasses:\"\n",
        "\n",
        "    # Generate a response using OpenAI GPT-3\n",
        "    response = openai.Completion.create(\n",
        "        engine='text-davinci-003',  # Specify the GPT-3 model to use\n",
        "        prompt=prompt,\n",
        "        temperature=0.5,  # Controls the randomness of the response\n",
        "        max_tokens=len(techniques),  # Generate tokens for each class option\n",
        "        # logprobs = 0.5,\n",
        "        n=1  # Generate a single response\n",
        "    )\n",
        "\n",
        "    # Extract the generated classifications from the response\n",
        "    print(response.choices)\n",
        "    classifications = response.choices[0].text.strip().split(\"\\n\")\n",
        "\n",
        "    return classifications\n",
        "\n",
        "# Example usage\n",
        "text_to_classify = \"In the course of their conversations, Patel discussed his desire to see a holy war between Muslims and non-Muslims.\"\n",
        "# class_options = ['Option 1', 'Option 2', 'Option 3']  # List of possible classes\n",
        "\n",
        "\n",
        "# Perform multi-class classification\n",
        "classified_labels = detect_techniques(text_to_classify, propaganda_techniques)\n",
        "print(\"Classified labels:\", classified_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0GXgMvkzd4t"
      },
      "outputs": [],
      "source": [
        "#add techniques\n",
        "\n",
        "train_df_copy = train_df.copy()\n",
        "\n",
        "for index, row in train_df.iterrows():\n",
        "    train_df_copy.at[index, 'emotions'] = new_column_value\n",
        "    train_df_copy.at[index, 'emotions_encoding'] = new_column_value"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}